{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9019b973",
   "metadata": {},
   "source": [
    "## Primera Llamada al Modelo\n",
    "\n",
    "En este ejercicio, aprenderemos a realizar nuestra primera llamada a un modelo de lenguaje usando GitHub Models API.\n",
    "\n",
    "# 1. GitHub Models API - Conexión Directa con OpenAI Client\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Configurar una conexión directa con GitHub Models usando el cliente OpenAI\n",
    "- Comprender los parámetros básicos de configuración de API\n",
    "- Implementar llamadas básicas a modelos de lenguaje\n",
    "- Aplicar mejores prácticas de seguridad con API keys\n",
    "\n",
    "## Introducción\n",
    "GitHub Models proporciona acceso a varios modelos de lenguaje mediante una API compatible con OpenAI. En este notebook aprenderemos a:\n",
    "1. Configurar el entorno y las credenciales\n",
    "2. Establecer una conexión con la API\n",
    "3. Realizar llamadas básicas al modelo\n",
    "4. Explorar diferentes parámetros de configuración\n",
    "\n",
    "## Configuración de Variables de Entorno\n",
    "\n",
    "Antes de ejecutar el código, asegúrate de tener configuradas las siguientes variables de entorno:\n",
    "\n",
    "```bash\n",
    "export GITHUB_BASE_URL=\"https://models.inference.ai.azure.com\"\n",
    "export GITHUB_TOKEN=\"tu_token_de_github_aqui\"\n",
    "```\n",
    "\n",
    "**Mejores Prácticas de Seguridad:**\n",
    "- Nunca hardcodees API keys en el código\n",
    "- Usa variables de entorno o archivos .env\n",
    "- No compartas credenciales en repositorios públicos\n",
    "- Rota las API keys regularmente\n",
    "\n",
    "## Instalación de Dependencias\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab37f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI library version: 1.100.0\n",
      "Python version: 3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "Base URL configurada: https://models.inference.ai.azure.com\n",
      "API Key configurada: ✓\n",
      "API Key preview: ghp_aW0sxG...HBnM\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Verificar que tenemos las bibliotecas correctas\n",
    "print(\"OpenAI library version:\", __import__('openai').__version__)\n",
    "print(\"Python version:\", __import__('sys').version)\n",
    "\n",
    "# Configuración del cliente OpenAI para GitHub Models\n",
    "try:\n",
    "    # Configurar el cliente con variables de entorno\n",
    "    client = OpenAI(\n",
    "        base_url=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    "    )\n",
    "    \n",
    "    # Verificar configuración (sin mostrar la API key completa por seguridad)\n",
    "    print(\"Base URL configurada:\", client.base_url)\n",
    "    print(\"API Key configurada:\", \"✓\" if client.api_key else \"✗\")\n",
    "    \n",
    "    if client.api_key:\n",
    "        print(\"API Key preview:\", client.api_key[:10] + \"...\" + client.api_key[-4:])\n",
    "    else:\n",
    "        print(\"⚠️  API Key no encontrada. Asegúrate de configurar GITHUB_TOKEN\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error en configuración: {e}\")\n",
    "    print(\"Verifica que las variables de entorno estén configuradas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a49b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la llamada: name 'client' is not defined\n",
      "Verifica tu configuración y conexión a internet\n"
     ]
    }
   ],
   "source": [
    "# Primera llamada básica al modelo\n",
    "def llamada_basica():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hola, ¿cómo estás? Responde en una oración.\"}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta del Modelo ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\n=== Información Técnica ===\")\n",
    "        print(f\"Modelo usado: {response.model}\")\n",
    "        print(f\"Tokens usados: {response.usage.total_tokens}\")\n",
    "        print(f\"Tokens de entrada: {response.usage.prompt_tokens}\")\n",
    "        print(f\"Tokens de salida: {response.usage.completion_tokens}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la llamada: {e}\")\n",
    "        print(\"Verifica tu configuración y conexión a internet\")\n",
    "\n",
    "# Ejecutar la función\n",
    "llamada_basica()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cccf4a",
   "metadata": {},
   "source": [
    "## Usando Roles del Sistema\n",
    "\n",
    "El rol \"system\" permite establecer el comportamiento y contexto del asistente antes de la conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "j1b57r1oeo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'client' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con mensaje de sistema\n",
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"Eres un experto en tecnología que explica conceptos complejos de manera simple y amigable. Siempre incluyes ejemplos prácticos.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"¿Qué es una API?\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta con Mensaje de Sistema ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar función\n",
    "usar_mensaje_sistema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a290e",
   "metadata": {},
   "source": [
    "## Explorando Parámetros de Configuración\n",
    "\n",
    "Los parámetros más importantes al hacer llamadas a LLMs son:\n",
    "\n",
    "- **temperature**: Controla la creatividad (0.0 = determinístico, 1.0 = muy creativo)\n",
    "- **max_tokens**: Límite de tokens en la respuesta\n",
    "- **model**: El modelo específico a usar (gpt-4o, gpt-3.5-turbo, etc.)\n",
    "- **messages**: Array de mensajes con roles (system, user, assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j48bg48xqs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando diferentes valores de temperature\n",
    "def comparar_temperature():\n",
    "    prompt = \"Escribe una historia muy corta sobre un robot que aprende a cocinar.\"\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 0.9]\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TEMPERATURE: {temp}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=100\n",
    "            )\n",
    "            \n",
    "            print(response.choices[0].message.content)\n",
    "            print(f\"\\nTokens usados: {response.usage.total_tokens}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fc272",
   "metadata": {},
   "source": [
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Diferentes Modelos\n",
    "Modifica el código para probar diferentes modelos disponibles (si tienes acceso):\n",
    "- gpt-4o\n",
    "- gpt-4o-mini\n",
    "- DeepSeek-R1-0528\n",
    "\n",
    "Revisa todos los modelos diponibles en la [documentación de Github Marketplace](https://github.com/marketplace?type=models)\n",
    "\n",
    "### Ejercicio 2: Crear un Asistente Especializado\n",
    "Diseña un mensaje de sistema para crear un asistente especializado en un tema específico (ejemplo: finanzas, salud, educación).\n",
    "\n",
    "### Ejercicio 3: Optimización de Tokens\n",
    "Experimenta con diferentes valores de max_tokens para encontrar el equilibrio entre respuesta completa y eficiencia de costos.\n",
    "\n",
    "## Conceptos Clave\n",
    "\n",
    "1. **Configuración segura** de APIs usando variables de entorno\n",
    "2. **Parámetros básicos** para controlar el comportamiento del modelo\n",
    "3. **Manejo de errores** en llamadas a APIs\n",
    "4. **Roles de mensajes** (system, user, assistant)\n",
    "5. **Monitoreo de uso** de tokens y costos\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el siguiente notebook exploraremos cómo LangChain simplifica y abstrae estas operaciones, proporcionando herramientas más poderosas para el desarrollo de aplicaciones con LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121434b",
   "metadata": {},
   "source": [
    "# Desarrollo de ejercicios ============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d126f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'client' is not defined\n"
     ]
    }
   ],
   "source": [
    "def usar_mensaje_sistema():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"\"\"Eres un experto en filosofia que buca ayudar y acompañar a estudiantes, \"\n",
    "                    explicas temas complicados con ejemplos faciles ,ademas trastas de que el alumno busque salir de la mala situacion.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"estoy triste\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(\"=== Respuesta con Mensaje de Sistema ===\")\n",
    "        print(response.choices[0].message.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar función\n",
    "usar_mensaje_sistema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
